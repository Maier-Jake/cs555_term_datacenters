\documentclass[12pt,letterpaper]{article}

% ===========================
% PACKAGES
% ===========================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage[font=small,labelfont=bf]{caption}

% ===========================
% HYPERREF SETUP
% ===========================
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    pdftitle={Do AI Data Centers Increase Residential Electricity Prices?},
    pdfauthor={Jake Maier, Eric Kearney},
    pdfsubject={CS555 Distributed Systems Term Project},
    pdfkeywords={distributed systems, Apache Spark, machine learning, data centers, electricity prices}
}

% ===========================
% CODE LISTING SETUP
% ===========================
\lstset{
    language=Scala,
    basicstyle=\small\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{white},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    frame=single,
    tabsize=2,
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    escapeinside={\%*}{*)}
}

% ===========================
% HEADER/FOOTER
% ===========================
\pagestyle{fancy}
\fancyhf{}
\setlength{\headheight}{15pt}
\rhead{CS555 Term Project}
\lhead{Maier \& Kearney}
\rfoot{Page \thepage}

% ===========================
% SECTION FORMATTING
% ===========================
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ===========================
% SPACING
% ===========================
\onehalfspacing

% ===========================
% TITLE PAGE INFO
% ===========================
\title{
    \vspace{1in}
    \textbf{\LARGE Do AI Data Centers Increase Residential Electricity Prices?}\\
    \vspace{0.5in}
    \large A Distributed Systems Approach to Energy Economics\\
    \vspace{1in}
    \normalsize CS555 Distributed Systems - Fall 2025\\
    Term Project Report
    \vspace{1in}
}

\author{
    \textbf{Jake Maier}\\
    \textit{Colorado State University}\\
    \texttt{jake.maier@colostate.edu}
    \and
    \textbf{Eric Kearney}\\
    \textit{Colorado State University}\\
    \texttt{eric.kearney@colostate.edu}
}

\date{December 2025}

% ===========================
% DOCUMENT BEGIN
% ===========================
\begin{document}

% TITLE PAGE
\maketitle
\thispagestyle{empty}
\newpage

% ABSTRACT
\begin{abstract}
\noindent
The explosion in Artificial Intelligence has brought with it an unprecedented
demand for increased computational infrastructure. Large technology companies
(e.g., Amazon, Google, Meta, Microsoft) have constructed hundreds of massive
data centers across the United States, with many more data center construction
projects currently underway.

This expansion has risen the question in the mind of analysts, investigative
journalists, and the people living near these data centers: \textbf{Are these
	power-hungry facilities driving up electricity costs for nearby residents?}

The question matters for several reasons:

\textbf{Economic Justice:} Rising energy costs disproportionately burden
low-income families. If data centers contribute to price increases, AI
advancement would be subsidized by vulnerable residential customers who have no
negotiating power with utilities and would likely gain little-to-none of the
gains associated with said advancement.

\textbf{Policy and Planning:} State and local governments face increasing
pressure to approve or deny data center development. Policymakers need
empirical evidence to inform zoning decisions, tax incentives, and
infrastructure planning.

\textbf{Climate Accountability:} Understanding the \textit{full cost} of AI
infrastructure includes impacts on surrounding communities. If data centers
rapidly increase residential electricity consumption and thus drive utilities
toward fossil fuel generation to quickly meet the growing demand, the climate
implications extend beyond direct facility emissions.

\textbf{Utility System Planning:} Electric utilities must balance competing
demands: accommodating large industrial customers (data centers) while
maintaining reliable, affordable service for residential customers.
Understanding price impacts helps utilities design appropriate rate structures
and investment strategies.
\end{abstract}

\newpage

% TABLE OF CONTENTS
\tableofcontents
\newpage

% START PAGE NUMBERING
\setcounter{page}{1}

% ===========================
% MAIN CONTENT
% ===========================

\section{Introduction}

\subsection{Problem Statement}
Can we quantify the impact of data centers on residential electricity prices?
This question requires processing large-scale datasets, engineering complex
temporal features, and training predictive models.

\section{Dominant Approaches to the Problem}

The challenge of analyzing data center impacts on electricity prices sits at the
intersection of spatial data analysis and econometric modeling. Several
approaches have emerged across these domains, each with their own trade-offs.

\textbf{Traditional Economic Analysis:}
Classical econometric studies \cite{angrist2009}\cite{bellemare2015} employ panel regression
methods with fixed effects to control for differences between states, utilities,
etc. These approaches excel at causal inference through techniques like
difference-in-differences but typically operate on smaller datasets processable
on single machines. Their limitation lies in scalability—analyzing millions of
utility-year observations with complex geographic relationships becomes
computationally prohibitive.

\textbf{Single-Machine Machine Learning:}
Standard ML workflows provide statistical outputs (p-values, confidence
intervals, etc.). However, these tools require all data to fit in the memory of
a single machine, limiting analysis to aggregated datasets. 

\textbf{Distributed Big Data Frameworks:}
Apache Spark and Hadoop enable horizontal scaling across clusters, supporting
datasets far exceeding single-machine capacity. Spark MLlib provides distributed
implementations of common algorithms. The advantage is scalability, though with
trade-offs: Gradient Boosted Trees and iterative methods may suffer from shuffle
bottlenecks (as we experienced).

\section{Methodology}

\subsection{Data Sources}

\textbf{Source 1: Electricity Pricing (EIA Form 861)}
\begin{itemize}
    \item Source: U.S. Energy Information Administration \texttt{\cite{eia2024}}
    \item Coverage: 2015--2024 (10 years), all US utilities
    \item Format: Annual Excel files
    \item Key fields: State, Year, Revenues (\$), Sales (MWh), Customers
\end{itemize}

\textbf{Source 2: Data Center Locations}
\begin{itemize}
    \item Sources: Company press releases \cite{aws_datacenters}
	\cite{google_datacenters} \cite{microsoft_datacenters}
	\cite{meta_datacenters}, Kaggle dataset \cite{kaggle_datacenters_2023} 
    \item Coverage: 93 facilities across 20 states (2006--2024)
    \item Operators: Amazon/AWS, Google, Microsoft, Meta, Apple, IBM, Oracle, others
    \item Key fields: State, Opening\_Year, Capacity\_MW, Latitude, Longitude
\end{itemize}

\subsection{Distributed Data Processing Pipeline}

\subsubsection{Local Data Preprocessing}

Before Spark processing, we preprocessed the raw EIA Excel files locally with
Python. We did this because:

\begin{itemize}
	\item The EIA Excel files were multi-sheet files with merged headers,
		footnotes, etc.
	\item Data across the years was inconsistent, with slightly different column
		names and/or ordering.
	\item The EIA data featured duplicates, null entries, etc. that required
		cleaning.
\end{itemize}

We made the determination that this data preparation process would be much
easier done in pandas than Spark. After preprocessing, the cleaned datasets were
uploaded to HDFS for distributed analysis.

\subsubsection{HDFS Data Processing}

After preprocessing and HDFS upload, we implemented a distributed feature
engineering pipeline in Spark (Scala) to construct the analysis dataset. 

\subsubsection{Temporal Grid Construction}

\textbf{Challenge:} Data centers open at irregular intervals, creating sparse
temporal data. Some states have no data center openings in certain years, yet we
need electricity prices for all state-year combinations to avoid selection bias.

\textbf{Solution:} Construct a complete state $\times$ year Cartesian product:

\begin{lstlisting}[language=Scala, caption={Complete Temporal Grid Construction}, label={lst:grid}]
// Determine observation window from power cost data
val minYear = powerCosts.agg(min("Year")).as[Int].first()  // 2013
val maxYear = powerCosts.agg(max("Year")).as[Int].first()  // 2024
val years = spark.range(minYear, maxYear + 1).toDF("Year")

// All states with at least one data center
val states = rawDatacenters.select("State").distinct()

// Cartesian product: 20 states x 12 years = 240 combinations
val stateYearGrid = states.crossJoin(years)
\end{lstlisting}

\textbf{Distributed Execution:}
\begin{itemize}
    \item \texttt{crossJoin}: Broadcasts smaller table (20 states) to all workers
    \item Each worker computes local Cartesian product
    \item Result: 240 state-year combinations partitioned across cluster
\end{itemize}

This ensures every state appears in every year, even if no data center opened that year (enabling proper time-series analysis).

\subsubsection{Cumulative Data Center Count}

\textbf{Challenge:} Random Forests need cumulative totals (``How many data centers exist in state X by year Y?''), not just openings per year.

\textbf{Solution:} Two-stage aggregation with window functions:

\begin{lstlisting}[language=Scala, caption={Cumulative Data Center Aggregation}, label={lst:cumulative}]
// Stage 1: Handle pre-period baseline
// (Data centers opened before 2013)
val prePeriod = rawDatacenters
  .filter($"Opening_Year" < minYear)
  .groupBy("State")
  .agg(count("*").alias("Pre_DC"))

// Stage 2: Count openings per year during observation period
val byYear = rawDatacenters
  .filter($"Opening_Year" >= minYear)
  .groupBy("State", "Opening_Year")
  .agg(count("*").alias("DC_Opened"))
  .withColumnRenamed("Opening_Year", "Year")

// Stage 3: Compute running total using window function
val w = Window.partitionBy("State").orderBy("Year")

val cumulativeDatacenters = stateYearGrid
  .join(byYear, Seq("State", "Year"), "left")
  .na.fill(0, Seq("DC_Opened"))              // Null = no openings
  .join(prePeriod, Seq("State"), "left")
  .na.fill(0, Seq("Pre_DC"))                  // Null = no pre-2013 DCs
  .withColumn("Cumulative_DC",                // Running sum
    $"Pre_DC" + sum($"DC_Opened").over(w))
  .drop("Pre_DC")
\end{lstlisting}

\textbf{Key Distributed Operations:}

\paragraph{Window Functions:}
\texttt{Window.partitionBy("State")} ensures:
\begin{itemize}
    \item Each state's cumulative sum computed independently
    \item Data for state X stays on same executor (partition locality)
    \item \texttt{orderBy("Year")} guarantees chronological aggregation
    \item \texttt{sum().over(w)} computes running total without shuffle
\end{itemize}

Example output for Virginia:
\begin{verbatim}
State  Year  DC_Opened  Cumulative_DC
VA     2013      0           2  (2 pre-2013 + 0 in 2013)
VA     2014      3           5  (2 pre-2013 + 3 cumulative)
VA     2015      0           5  (no change)
VA     2016      1           6  (one more opened)
...
\end{verbatim}

\paragraph{Join Strategy:}
\begin{itemize}
    \item \textbf{Left joins} preserve all 240 state-year combinations
    \item \texttt{na.fill(0)} replaces \texttt{null} with zero (no data centers opened)
    \item Alternative: Inner join would drop years with no openings (creates bias)
\end{itemize}

\subsubsection{Electricity Price Integration}

\begin{lstlisting}[language=Scala, caption={Price Data Integration}, label={lst:prices}]
// Aggregate prices to state-year level
val prices = powerCosts
  .groupBy("State", "Year")
  .agg(avg("Price_Per_kWh").alias("Avg_kWh"))

// Join prices to cumulative data center counts
val dcFull = cumulativeDatacenters
  .join(prices, Seq("State", "Year"), "left")
  .na.drop("any", Seq("Avg_kWh"))  // Drop rows missing prices
\end{lstlisting}

\textbf{Distributed Execution:}
\begin{itemize}
    \item \texttt{groupBy}: Hash-partitions by \texttt{(State, Year)} across workers
    \item \texttt{avg()}: Each partition computes local averages
    \item \texttt{join()}: Co-partitioned join (both sides have same partition key)
    \item Result: No shuffle required, data stays local
\end{itemize}

\textbf{Design Decision:} We aggregate multiple utilities per state into a single state-level average. 

\subsubsection{Advanced Feature Engineering}

\begin{lstlisting}[language=Scala, caption={State Categorical Encoding}, label={lst:encoding}]
// Transform state names to indices
val stateIndexer = new StringIndexer()
  .setInputCol("State")
  .setOutputCol("StateIndex") .setHandleInvalid("keep")

// Convert indices to one-hot vectors
val stateEncoder = new OneHotEncoder()
  .setInputCol("StateIndex")
  .setOutputCol("StateVec")
\end{lstlisting}

\textbf{Rationale:} States have different baseline electricity prices due to:
\begin{itemize}
    \item Fuel mix (coal vs. nuclear vs. renewables)
    \item Regulation (deregulated markets vs. monopolies)
    \item Geography (transmission distances)
\end{itemize}

\paragraph{Temporal Lag Features:}

\begin{lstlisting}[language=Scala, caption={Lagged Price Feature}, label={lst:lag}]
val w = Window.partitionBy("State").orderBy("Year")

val dcWithPrev = dcFull
  .withColumn("PrevPrice", lag("Avg_kWh", 1).over(w))
  .na.drop("any", Seq("PrevPrice"))
\end{lstlisting}

\textbf{Purpose:} \texttt{PrevPrice} captures temporal autocorrelation—prices are correlated year-over-year due to long-term contracts, infrastructure inertia. Including \texttt{lag(price, 1)} helps model distinguish:
\begin{itemize}
    \item \textbf{Trend}: General price inflation over time
    \item \textbf{Shock}: Sudden changes due to data center openings
\end{itemize}

\textbf{Distributed Execution:}
\begin{itemize}
    \item \texttt{lag()}: Accesses previous row \textit{within same partition}
    \item Window sorted by \texttt{Year} ensures chronological ordering
    \item Each executor processes states independently
    \item First year per state has \texttt{null} (no prior year) → dropped
\end{itemize}

\subsubsection{Final Dataset Schema}

After feature engineering, the analysis dataset contains:

\begin{table}[h]
\centering
\caption{Final Spark Dataset Schema}
\begin{tabular}{lll}
\hline
\textbf{Column} & \textbf{Type} & \textbf{Description} \\
\hline
State & String & Two-letter state code \\
Year & Integer & Observation year (2013-2024) \\
Cumulative\_DC & Integer & Total data centers by year \\
DC\_Opened & Integer & New data centers this year \\
Avg\_kWh & Double & Average electricity price (\$/kWh) \\
StateIndex & Integer & Encoded state ID (0-19) \\
StateVec & Vector & One-hot encoded state (19 dims) \\
PrevPrice & Double & Prior year's price \\
\hline
\end{tabular}
\end{table}

\textbf{Dimensions:} 240 observations (20 states $\times$ 12 years), 8 features

\subsection{Machine Learning Implementation}

\textbf{Algorithm Selection Journey:}

Our project evolved through three algorithm attempts, each revealing different distributed systems trade-offs:

\textbf{Attempt 1: Gradient Boosted Trees (FAILED)}
\begin{itemize}
    \item \textbf{Why We Tried:} Best predictive accuracy, automatic feature interactions
    \item \textbf{Implementation:} Spark MLlib's \texttt{GBTRegressor}
    \item \textbf{Failure Mode:} Persistent \texttt{MetadataFetchFailedException} during iterative boosting
    \item \textbf{Root Cause:} We believe worker node(s) were being overloaded
		and crashing, causing the training process to fail.
    \item \textbf{Attempted Fixes:} Reduced iterations, increased executor memory, adjusted shuffle partitions
    \item \textbf{Outcome:} Could not resolve before deadline
\end{itemize}

\textbf{Attempt 2: Linear Regression (FAILED)}
\begin{itemize}
    \item \textbf{Why We Tried:} Simpler iterative algorithm, provides coefficients for interpretation
    \item \textbf{Implementation:} Spark MLlib's \texttt{LinearRegression} with gradient descent
    \item \textbf{Failure Mode:} Same \texttt{MetadataFetchFailedException}
\end{itemize}

\textbf{Attempt 3: Random Forest (SUCCESS)}
\begin{itemize}
    \item \textbf{Why We Tried:} Single-pass tree construction, minimal shuffles
    \item \textbf{Implementation:} Spark MLlib's \texttt{DecisionTreeRegressor}
    \item \textbf{Success Factors:} 
    \begin{itemize}
        \item Tree built in one distributed pass (not iterative)
        \item Workers compute best splits locally, report to master
        \item Minimal shuffle—only for aggregating split statistics
    \end{itemize}
    \item \textbf{Trade-off:} Less accurate than GBT, no p-values like Linear Regression
    \item \textbf{Outcome:} Successful training and prediction
\end{itemize}

\textbf{Key Learning:} Algorithm selection in distributed systems depends not just on accuracy, but on communication patterns. Iterative algorithms with many shuffle rounds may be infeasible even on modest-sized datasets.

\textbf{Final Model Training:}
\begin{lstlisting}[language=Scala]
val rf = new RandomForestRegressor()
  .setFeaturesCol("features")
  .setLabelCol("label")
  .setNumTrees(100)
  .setMaxDepth(2)
  .setMinInstancesPerNode(5)
  .setSubsamplingRate(1.0)

// Hyper-parameter tuning, this is fine because we have a tiny dataset.
val paramGrid = new ParamGridBuilder()
  .addGrid(rf.numTrees, Array(125, 50, 100, 200, 300))
  .addGrid(rf.maxDepth, Array(2, 3, 4, 6, 8, 10))
  .build()
\end{lstlisting}

\subsection{Distributed Evaluation}

\textbf{Parallel Metrics Computation:}
\begin{lstlisting}[language=Scala]
val evaluator = new RegressionEvaluator()
  .setLabelCol("label")
  .setPredictionCol("prediction")
  .setMetricName("rmse")
\end{lstlisting}

\section{Experimental Benchmarks}

\subsection{Model Performance}

\begin{table}[h]
\centering
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Root Mean Square Error (RMSE) & \$0.00571/kWh \\
Mean Absolute Error (MAE) & \$0.0122/kWh \\
$R^2$ (Variance Explained) & 0.7651 (76.51\%) \\
\midrule
State Importance & 0.0073 \\
Previous Price Importance & 0.6518 \\
\textbf{Data Center Importance} & 0.0210 \\
\midrule
\multicolumn{2}{l}{\textbf{Economic Interpretation:}} \\
Average household (10,000 kWh/year) & ±\$57/year prediction error \\
Typical residential price & $\sim$\$0.12/kWh \\
RMSE as \% of average price & $\sim$12.8\% \\
\bottomrule
\end{tabular}
\caption{Random Forest model performance on held-out test set (48 observations, 20\% of data)}
\end{table}

\textbf{Interpretation of $R^2 = 0.765$:}

Our model explains 76.5\% of price variation, with the price of electricity in
the previous year accounting for the vast majority (65\%) of those prices, and
data center openings accounting for a comparatively much smaller 2\%.
Electricity prices depend on many factors (natural gas costs, weather,
regulation) beyond data centers. However, we have shown that the number of
nearby data centers does have a measurable (albeit small) impact on residential
electricity prices.

\subsection{Feature Importance Analysis}

Our Random Forest model has revealed the following about which features most
strongly predict electricity prices:

\begin{itemize}
	\item \textbf{PrevPrice - Previous Price (65.2\%):} The previous year's
	electricity price is by far the strongest predictor. This is consistent with
	an understanding that these utility companies are typically heavily
	regulated and sign long-term contracts that forbid them from increasing
	their prices drastically year-over-year.
	\item \textbf{StateVec - The State being analyzed (0.7\%):} State differences
	contribute minimally, this is contrary to one of our hypotheses.
	\item \textbf{DC\_Opened - Whether a Data Center has opened nearby (2.1\%):}
	Data Center openings explain a more modest than expected, but measurable impact on residential
	electricity prices.
\end{itemize}

\section{Insights Gleaned}

\subsection{Data Insights}

While we were unable to show that data center openings are a large driver in
rising electricity prices, we can still plainly see from the data that
electricity prices are rising faster in states that have more data centers than
states with fewer data centers.

Figure~\ref{fig:arizona} shows electricity price trends in Arizona, one of the states with significant data center growth during our study period.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{figures/AZ.png}
\caption{Arizona electricity prices (2014--2022) with counterfactual scenarios
	showing different data center growth assumptions. The black line represents
		actual observed prices. The divergence between actual and baseline
		scenarios after 2019 is suggestive of data center impact, though other
		factors cannot be ruled out, this visualization demonstrates a positive
		correlation between number of data centers and electricity prices.}
\label{fig:arizona}
\end{figure}

Figure~\ref{fig:highvslow} Compares electricity prices in two 'high' data center
	states against a 'low' data center state.

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{figures/highvslow.png}
\caption{Compares electricity prices in two 'high' data center
	states (Virginia and Texas) against a 'low' data center state (Wyoming)
	and demonstrates the cost of electricity is growing significantly faster in
	these 'high' data center states.}
\label{fig:highvslow}
\end{figure}


\subsection{Distributed Systems Insights}

\textbf{Data Partitioning Strategy:}
Our window function approach (partition by state) enabled parallel computation
of cumulative metrics. Alternative approaches (global sorting) would have
required expensive all-to-all shuffles.

\textbf{Fault Tolerance Through HDFS:}
3x replication in HDFS ensured no data loss. Spark's RDD lineage allowed
automatic recomputation when intermediate results were lost during debugging.

\textbf{Limitations:}
\begin{itemize}
    \item Random Forest provides predictions but not statistical significance (p-values)
    \item Missing control variables, especially natural gas prices which could confound results
    \item Correlation $\neq$ causation, future work requires stronger causal inference methods
\end{itemize}

\section{How the Problem Space Will Look in the Future}

\textbf{Explosive Data Center Growth:}
Industry projections suggest US data center electricity consumption could reach
8-12\% of total generation by 2030 \cite{doe2024}, up from 3-4\% in 2024. This
growth driven by large language models, AI training clusters, and edge
computing. This will intensify the importance of the questions our project seeks
to addresses.

\textbf{Smart Grid and Real-Time Data:}
Advanced Metering Infrastructure (AMI) deployments are generating unprecedented
granularity: hourly consumption data for millions of customers, real-time
pricing signals, and substation-level load monitoring. Future analyses will be
able to shift from annual state-level aggregates to minute-by-minute
utility-specific models. This will provide much greater data granularity but
will also require even more sophisticated distributed processing; further
exacerbating the need for distributed systems in analyzing these problems.

\textbf{Edge AI and Distributed Training:}
The computational paradigm is shifting from centralized mega-data-centers to
distributed edge deployments. This geographic dispersion complicates impact
analysis; instead of a few large facilities per state, it will become necessary
to track thousands of smaller edge nodes to get the full picture. 

\textbf{Policy-Driven Transparency:}
Growing regulatory pressure (e.g., proposed data center disclosure mandates in
Virginia, Texas) will improve data availability \cite{virginia2025}
\cite{texas2025}. Utilities may be
required to report data center loads separately, this transparency would enable
causal analyses currently impossible.

\section{Conclusions}

\subsection{Model Results Summary}

\textbf{Performance:} RMSE = \$0.00571/kWh, $R^2 = 0.765$

\textbf{Interpretation:} Data center metrics explain $\sim$18\% of residential electricity price variation across states. This establishes a predictive relationship, though causal claims require additional analysis.

\textbf{Trade-offs:} We sacrificed coefficient interpretation (Linear Regression) and accuracy (GBT) to achieve successful distributed execution (Random Forest). This reflects real-world engineering constraints when deploying ML at scale.

\subsection{Future Work}

\textbf{Resolve Shuffle Failures:}
\begin{itemize}
    \item Debug \texttt{MetadataFetchFailedException} with increased logging
    \item Experiment with alternative shuffle managers (e.g., Tungsten sort shuffle)
    \item Implement Linear Regression to obtain coefficients and p-values
\end{itemize}

\textbf{Expand Dataset:}
\begin{itemize}
    \item Add control variables (natural gas prices, weather, population)
    \item Increase temporal resolution (monthly instead of annual)
    \item Include more states and recent data (2024--2025)
\end{itemize}

\textbf{Advanced Distributed Techniques:}
\begin{itemize}
    \item Implement difference-in-differences estimation using Spark SQL
    \item Distributed hyperparameter tuning (parallel grid search)
    \item Ensemble methods (Random Forest) for improved predictions
\end{itemize}

\newpage

\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{99}

\bibitem{angrist2009} Angrist, J.D., \& Pischke, J.S. (2009). \textit{Mostly Harmless Econometrics: An Empiricist's Companion}. Princeton University Press.

\bibitem{bellemare2015} Bellemare, M.F. (2015). On R-squared in applied economics. Blog post. \url{https://marcfbellemare.com/wordpress/10793}

\bibitem{eia2024} U.S. Energy Information Administration. (2024). Form EIA-861 Detailed Data Files (2015--2024). Retrieved from \url{https://www.eia.gov/electricity/data/eia861/}

\bibitem{cohen1988} Cohen, J. (1988). \textit{Statistical Power Analysis for the Behavioral Sciences} (2nd ed.). Hillsdale, NJ: Lawrence Erlbaum Associates.

\bibitem{dean2004} Dean, J., \& Ghemawat, S. (2004). MapReduce: Simplified data processing on large clusters. In \textit{OSDI'04: Sixth Symposium on Operating System Design and Implementation} (pp. 137--150).

\bibitem{jones2018} Jones, N. (2018). How to stop data centres from gobbling up the world's electricity. \textit{Nature, 561}(7722), 163--166.

\bibitem{masanet2020} Masanet, E., Shehabi, A., Lei, N., Smith, S., \& Koomey, J. (2020). Recalibrating global data center energy-use estimates. \textit{Science, 367}(6481), 984--986.

\bibitem{zaharia2016} Zaharia, M., Xin, R.S., Wendell, P., Das, T., Armbrust, M., Dave, A., ... \& Stoica, I. (2016). Apache Spark: A unified engine for big data processing. \textit{Communications of the ACM, 59}(11), 56--65.

\bibitem{virginia2025} Virginia Mercury. (2025, January 15). As data center
boom continues, Va. legislators broach new regulations. Retrieved from
\url{https://virginiamercury.com/2025/01/14/as-data-center-boom-continues-va-legislators-broach-new-regulations/}

\bibitem{texas2025} Bell Nunnally \& Martin LLP. (2025, June 25). Texas
Legislature Passes Important Regulatory Changes for Texas Data Centers and
Other Large Load Customers. Retrieved from
\url{https://www.bellnunnally.com/news/texas-legislature-passes-important-regulatory-changes-for-texas-data-centers-and-other-large-load-customers/}

\bibitem{kaggle2023} Shivam, M. (2023). Data Center Locations of Top Tech Companies. Kaggle dataset. \url{https://www.kaggle.com/datasets/mauryansshivam/list-of-data-centers-of-top-tech-companies}

\bibitem{jlarc2024} Joint Legislative Audit and Review Commission. (2024,
December). Data Centers in Virginia. Virginia General Assembly.

\bibitem{doe2024} U.S. Department of Energy. (2024, December 20).
DOE Releases New Report Evaluating Increase in Electricity Demand from
Data Centers. Retrieved from
\url{https://www.energy.gov/articles/doe-releases-new-report-evaluating-increase-electricity-demand-data-centers}

\bibitem{eia861_2024}
U.S. Energy Information Administration. (2024). \textit{Form EIA-861 Detailed Data Files}. U.S. Department of Energy. Retrieved from \url{https://www.eia.gov/electricity/data/eia861/}

\bibitem{kaggle_datacenters_2023}
Maurya, S. (2023). \textit{Data Center Locations of Top Tech Companies}. Kaggle. Retrieved from \url{https://www.kaggle.com/datasets/mauryansshivam/list-of-data-centers-of-top-tech-companies}

\bibitem{aws_datacenters}
Amazon Web Services. (2024). \textit{AWS Global Infrastructure}. Retrieved from \url{https://aws.amazon.com/about-aws/global-infrastructure/}

\bibitem{google_datacenters}
Google Cloud. (2024). \textit{Google Data Center Locations}. Retrieved from \url{https://www.google.com/about/datacenters/locations/}

\bibitem{microsoft_datacenters}
Microsoft Azure. (2024). \textit{Azure Datacenters}. Retrieved from \url{https://datacenters.microsoft.com/}

\bibitem{meta_datacenters}
Meta Platforms. (2024). \textit{Meta Data Center Infrastructure}. Retrieved from \url{https://sustainability.fb.com/data-centers/}

\end{thebibliography}

\end{document}
